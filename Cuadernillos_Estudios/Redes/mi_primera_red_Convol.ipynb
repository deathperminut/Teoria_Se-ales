{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LIBRERIAS REDES NEURONALES CONVOLUCIONALES\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dropout,MaxPooling2D,Flatten,Dense\n",
    "################################################################################\n",
    "\n",
    "\n",
    "##DATA\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "########################################################################\n",
    "\n",
    "##LIBRERIAS PARA TRATAMIENTO GRAFICO Y MATEMATICO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDESCARGAMOS LA INFORMACIÓN A NUESTRA NOTEBOOK\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\"\"\"\n",
    "\n",
    "DESCARGAMOS LA INFORMACIÓN A NUESTRA NOTEBOOK\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### REALIZAREMOS ANALISIS EXPLORATORIO\n",
    "train_images.shape ###TENEMOS 60000 IMAGENES REPRESENTADAS EN UNA MATRIZ DE 28,28 QUE SERIAN PIXELES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b8379bfd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[99]) ### COMANDO PARA GRAFICAR IMAGENES \n",
    "## COMO PODEMOS OBSERVAR ES UN ALMACEN DE ROPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIMPIEZA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCOMO ESTAMOS TRABAJANDO EN CATEGORIAS COMO SALIDA, LO MEJOR ES TRABAJARLO DE FORMA VECTORIAL.\\nEN UN SOLO ARREGLO DE CEROS Y UNOS.\\n\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## AHORA VAMOS A LIMPIAR NUESTROS DATOS\n",
    "\n",
    "train_images = train_images.astype('float32') / 255 ##CONVERTIMOS EN FLOTANTE PARA QUE LA RED LOS PUEDA TRABAJAR\n",
    "# MAS FACILMENTE Y SOBRE 255 DEBIDO A QUE LOS PIXELES QUE MANEJAN LAS IMAGENES VAN DE 1 A 255 \n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "EL PRIMER PARAMETRO ES EL CONJUNTO TOTAL DE IMAGENES OSEA 60000,\n",
    "DESPUES VIENEN LOS 28*28, Y EL ULTIMO 1 SE REFIERE A QUE SOLO ES UN CANAL\n",
    "YA QUE ESTAMOS TRABAJANDO CON ESCALA DE GRISES, SINO SERIAN 3 CANALES AL ADMITIR LOS DEMAS COLORES.\n",
    "\"\"\"\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)## TENGO SOLO 10 POSIBLES OPCIONES \n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "\"\"\"\n",
    "\n",
    "COMO ESTAMOS TRABAJANDO EN CATEGORIAS COMO SALIDA, LO MEJOR ES TRABAJARLO DE FORMA VECTORIAL.\n",
    "EN UN SOLO ARREGLO DE CEROS Y UNOS.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHORA VAMOS A CREAR NUESTRO MODELO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 64)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 14, 14, 32)        8224      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               401664    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 412,778\n",
      "Trainable params: 412,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "\"\"\"\n",
    "PARAMETROS IMPORTANTES EN UNA RED CONVOLUCIONAL, PRIMERO ESTA GENERALMENTE ES LA RED DE ENTRADA, \n",
    "AQUI TENEMOS:\n",
    "filter{int} : NUMERO DEL FILTRO\n",
    "kernel_size{int}\n",
    "input_shape=(COMO VOY A RECIBIR LOS DATOS POR EJEMPLAR, NO EN CONJUNTO POR ESO SERIA MATRIZ DE 28,28 DE UN SOLO\n",
    "CANAL)\n",
    "\"\"\"\n",
    "### AHORA APILAMOS CAPAS\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3)) ## REDUCIR OVERFITTING\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())## APLANA LA MATRIZ O TENSORES PARA QUE QUEDENE EN SERIE\n",
    "## Y YA CON VECTORES PLANOS, YA PUEDO UTILIZAR CAPAS DENSAS COMO VENIAMOS TRAJABAJANDO ANTES\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax')) ## 10 PORQUE SON 10 OPCIONES\n",
    "## DE TIPO SOFTMAX LA DE SALIDA PORQUE ME DA LA PROBABILIDAD PARA CADA CLASE\n",
    "## SUMANDOLAS ME DA UNO ENTONCES ESCOJO LA DE MAYOR PROBABILIDAD\n",
    "# Take a look at the model summary\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILAR-ENTRENAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPILAMOS EL MODELO\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy']) ### CATEGORICAL_CROSSENTROPY DEBIDO A QUE ESTAMOS EN UN MODELO DE CLASIFICACIÓN\n",
    "\n",
    "## ADEMAS EXISTEN MULTIPLES OPCIONES, POR LO TANTO CATEGORICAL ES IDEAL\n",
    "## SI FUERAN SOLO 2 SERIAS BINARY CROSSENTROPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 107s 111ms/step - loss: 0.5741 - accuracy: 0.7904\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 104s 111ms/step - loss: 0.3933 - accuracy: 0.8595\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 110s 117ms/step - loss: 0.3525 - accuracy: 0.8741\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 86s 92ms/step - loss: 0.3332 - accuracy: 0.8798\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 85s 90ms/step - loss: 0.3257 - accuracy: 0.8855\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 90s 96ms/step - loss: 0.3166 - accuracy: 0.8886\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 109s 116ms/step - loss: 0.3148 - accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 94s 100ms/step - loss: 0.3109 - accuracy: 0.8906\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 84s 89ms/step - loss: 0.3131 - accuracy: 0.8914\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 103s 110ms/step - loss: 0.3117 - accuracy: 0.8919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22b80342cd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         batch_size=64,\n",
    "         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3219927251338959, 0.8867999911308289]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07614c21755fca2e5437534253816bd89da6526460ae43611f5bb73c32569538"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
